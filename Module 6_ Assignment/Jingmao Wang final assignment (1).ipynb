{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f80196",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ba3aad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e8d1803",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader   #import own data\n",
    "\n",
    "corpus_root = 'D:/1/EAI6000/6'\n",
    "file_pattern = 'BIRDS AND MAN.txt'\n",
    "corpus = PlaintextCorpusReader(corpus_root, file_pattern)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0e2bf5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Project', 'Gutenberg', 'EBook', 'of', 'Birds', ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.words() # Returns a list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c3a6386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83698"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus.words()) # No.of words in the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d044fd9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'Project', 'Gutenberg', 'EBook', 'of', 'Birds', 'and', 'Man', ',', 'by', 'W', '.', 'H', '.', 'Hudson'], ['This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', '.'], ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.sents(fileids='BIRDS AND MAN.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6476cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'Project', 'Gutenberg', 'EBook', 'of', 'Birds', 'and', 'Man', ',', 'by', 'W', '.', 'H', '.', 'Hudson'], ['This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', '.'], ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.sents() # Returns a list of list of strings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d419465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus.fileids())  # only have one book "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29021424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BIRDS AND MAN.txt']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.fileids()  # all book name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a523d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg EBook of Birds and Man, by W. H. Hudson\r\n",
      "\r\n",
      "This eBook is for the use of anyone anywhere at no cost and with\r\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\r\n",
      "re-use it under the terms of the Project Gutenberg License included\r\n",
      "with this eBook or online at www.gutenberg.org\r\n",
      "\r\n",
      "\r\n",
      "Title: Birds and Man\r\n",
      "\r\n",
      "Author: W. H. Hudson\r\n",
      "\r\n",
      "Release Date: October 18, 2011 [EBook #37787]\r\n",
      "\r\n",
      "Language: English\r\n",
      "\r\n",
      "Character set encoding: ASCII\r\n",
      "\r\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK BIRDS AND MAN ***\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Produced by Chris Curnow, Tom Cosmas and the Online\r\n",
      "Distributed Proofreading Team at http://www.pgdp.net (This\r\n",
      "file was produced from images generously made available\r\n",
      "by The Internet Archive)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "BIRDS AND MAN\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "  +----------------------------+\r\n",
      "  |    _BY THE SAME AUTHOR_    |\r\n",
      "  |                            |\r\n",
      "  | Birds in a Village         |\r\n",
      "  |                            |\r\n",
      "  | Adventures among Birds     |\r\n",
      "  |              \n"
     ]
    }
   ],
   "source": [
    "print(corpus.raw('BIRDS AND MAN.txt').strip()[:1000])  # First 1000 characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb2590a",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca1a8873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tThe Project Gutenberg EBook of Birds and Man, by W. H. Hudson\r\n",
      "1:\t\r\n",
      "2:\tThis eBook is for the use of anyone anywhere at no cost and with\r\n",
      "3:\talmost no restrictions whatsoever.  You may copy it, give it away or\r\n",
      "4:\tre-use it under the terms of the Project Gutenberg License included\r\n",
      "5:\twith this eBook or online at www.gutenberg.org\r\n",
      "6:\t\r\n",
      "7:\t\r\n",
      "8:\tTitle: Birds and Man\r\n",
      "9:\t\r\n",
      "10:\tAuthor: W. H. Hudson\r\n"
     ]
    }
   ],
   "source": [
    "for i, line in enumerate(corpus.raw('BIRDS AND MAN.txt').split('\\n')):\n",
    "    if i > 10: \n",
    "        break\n",
    "    print(str(i) + ':\\t' + line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b61c23c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This eBook is for the use of anyone anywhere at no cost and with\\r', 'almost no restrictions whatsoever.  You may copy it, give it away or\\r', 're-use it under the terms of the Project Gutenberg License included\\r', 'with this eBook or online at www.gutenberg.org\\r']\n"
     ]
    }
   ],
   "source": [
    "sent = corpus.raw('BIRDS AND MAN.txt').split('\\n')[2:6]\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbc26910",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca6b8a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"['This eBook is for the use of anyone anywhere at no cost and with\\\\r', 'almost no restrictions whatsoever.\",\n",
       " \"You may copy it, give it away or\\\\r', 're-use it under the terms of the Project Gutenberg License included\\\\r', 'with this eBook or online at www.gutenberg.org\\\\r']\"]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(str(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b73740a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', \"'This\", 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with\\\\r', \"'\", ',', \"'almost\", 'no', 'restrictions', 'whatsoever', '.']\n",
      "['You', 'may', 'copy', 'it', ',', 'give', 'it', 'away', 'or\\\\r', \"'\", ',', \"'re-use\", 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included\\\\r', \"'\", ',', \"'with\", 'this', 'eBook', 'or', 'online', 'at', 'www.gutenberg.org\\\\r', \"'\", ']']\n"
     ]
    }
   ],
   "source": [
    "for sent in sent_tokenize(str(sent)):\n",
    "    print(word_tokenize(str(sent)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04e8f5e",
   "metadata": {},
   "source": [
    "## Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e9dd67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"You may copy it, give it away or\\\\r', 're-use it under the terms of the Project Gutenberg License included\\\\r', 'with this eBook or online at www.gutenberg.org\\\\r']\"]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63ca26bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'may', 'copy', 'it', ',', 'give', 'it', 'away', 'or\\\\r', \"'\", ',', \"'re-use\", 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included\\\\r', \"'\", ',', \"'with\", 'this', 'ebook', 'or', 'online', 'at', 'www.gutenberg.org\\\\r', \"'\", ']']\n"
     ]
    }
   ],
   "source": [
    "for sent in sent_tokenize(sent):\n",
    "    print([word.lower() for word in word_tokenize(sent)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c614f127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'may', 'copy', 'it', ',', 'give', 'it', 'away', 'or\\\\r', \"'\", ',', \"'re-use\", 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included\\\\r', \"'\", ',', \"'with\", 'this', 'eBook', 'or', 'online', 'at', 'www.gutenberg.org\\\\r', \"'\", ']']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(sent)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a98ed4",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ec8078b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_en = stopwords.words('english')\n",
    "print(stopwords_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2170a784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'may', 'copy', 'it', ',', 'give', 'it', 'away', 'or\\\\r', \"'\", ',', \"'re-use\", 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included\\\\r', \"'\", ',', \"'with\", 'this', 'ebook', 'or', 'online', 'at', 'www.gutenberg.org\\\\r', \"'\", ']']\n"
     ]
    }
   ],
   "source": [
    "sent_tokenized_lowered = list(map(str.lower, word_tokenize(sent)))\n",
    "print(sent_tokenized_lowered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ded64dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['may', 'copy', ',', 'give', 'away', 'or\\\\r', \"'\", ',', \"'re-use\", 'terms', 'project', 'gutenberg', 'license', 'included\\\\r', \"'\", ',', \"'with\", 'ebook', 'online', 'www.gutenberg.org\\\\r', \"'\", ']']\n"
     ]
    }
   ],
   "source": [
    "stopwords_en = set(stopwords.words('english')) \n",
    "\n",
    "# List comprehension.\n",
    "print([word for word in sent_tokenized_lowered if word not in stopwords_en])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06f3eeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From string.punctuation: <class 'str'> !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "# It's a string so we have to them into a set type\n",
    "print('From string.punctuation:', type(punctuation), punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d641af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doing', 'in', '$', \"won't\", 'same', 'should', 'before', 'mustn', '%', 'herself', 'they', 'below', 'ours', 'theirs', 'been', 'under', '^', \"'\", \"aren't\", 'yourself', 'both', 'myself', 'does', 'was', 'ma', '<', 'were', \"don't\", 'for', 'if', 'that', 'between', 'when', '?', 'he', 'all', 'out', 're', 'being', \"needn't\", 'are', 'our', 'few', \"weren't\", '_', 'whom', 'these', \"shan't\", 'ain', 'their', \"hadn't\", \"isn't\", \"you're\", 'of', '{', 'can', 'with', ':', \"you'll\", 'be', 'yours', 'yourselves', \"you've\", 'ourselves', 'at', 'too', 'those', 'why', 'off', 'won', 'who', 'where', 'which', '>', 'me', 'there', 'shouldn', '@', 'itself', \"wasn't\", 've', '|', 'such', 'themselves', 'above', '&', \"haven't\", 'by', 'not', 'them', 'up', 'this', 'my', 'how', 'here', 'more', 'from', 'so', 'didn', \"you'd\", '=', 'because', 'very', 'himself', 'couldn', ']', 'shan', 'and', 'down', '!', '*', 'only', 'just', \"didn't\", 'her', 'doesn', 'you', 'an', 'on', 'did', 'having', 'y', 'will', 'now', 'any', 'some', \"mustn't\", '-', '}', 'but', 'nor', 'we', \"wouldn't\", ',', '`', 'over', 'm', 'she', 'has', 'his', 'needn', 'its', 'own', 'weren', 'hadn', 'again', \"should've\", 'what', 't', 'the', 'is', 'into', 'don', \"doesn't\", 'am', 'hasn', ';', 'had', 'than', 'once', '\"', '[', 'i', '\\\\', 'through', \"shouldn't\", \"that'll\", 'then', 'or', '~', 'do', \"it's\", 'after', 'him', 'about', 'no', 'isn', 'wouldn', '+', 'while', \"hasn't\", 'hers', 'other', '#', 'most', 'against', \"couldn't\", 'to', 'd', 'll', 'mightn', 'it', 'o', \"mightn't\", 'as', \"she's\", 'have', 'further', 'until', '.', '/', 's', 'haven', 'your', 'aren', 'a', '(', 'wasn', ')', 'during', 'each'}\n"
     ]
    }
   ],
   "source": [
    "stopwords_en_withpunct = stopwords_en.union(set(punctuation)) #Combining the punctuation with the stopwords from NLTK\n",
    "print(stopwords_en_withpunct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c8a64840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['may', 'copy', 'give', 'away', 'or\\\\r', \"'re-use\", 'terms', 'project', 'gutenberg', 'license', 'included\\\\r', \"'with\", 'ebook', 'online', 'www.gutenberg.org\\\\r']\n"
     ]
    }
   ],
   "source": [
    "print([word for word in sent_tokenized_lowered if word not in stopwords_en_withpunct]) #  Removing stopwords with punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25d9b57",
   "metadata": {},
   "source": [
    "## Using a stronger/longer list of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "608aa978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords from stopwords-json\n",
    "stopwords_json = {\"en\":[\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"]}\n",
    "stopwords_json_en = set(stopwords_json['en'])\n",
    "stopwords_nltk_en = set(stopwords.words('english'))\n",
    "stopwords_punct = set(punctuation)\n",
    "# Combine the stopwords. Its a lot longer so I'm not printing it out...\n",
    "stoplist_combined = set.union(stopwords_json_en, stopwords_nltk_en, stopwords_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fee584b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With combined stopwords:\n",
      "['copy', 'give', 'or\\\\r', \"'re-use\", 'terms', 'project', 'gutenberg', 'license', 'included\\\\r', \"'with\", 'ebook', 'online', 'www.gutenberg.org\\\\r']\n"
     ]
    }
   ],
   "source": [
    "# Remove the stopwords from `single_no8`.\n",
    "print('With combined stopwords:')\n",
    "print([word for word in sent_tokenized_lowered if word not in stoplist_combined])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fa4f06",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "85723aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walk\n",
      "walk\n",
      "walk\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer     # make some example\n",
    "porter = PorterStemmer()\n",
    "\n",
    "for word in ['walking', 'walks', 'walked']:\n",
    "    print(porter.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ed90af72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walking\n",
      "walk\n",
      "walked\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer # make some example\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "for word in ['walking', 'walks', 'walked']:\n",
    "    print(wnl.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "70e4985a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you',\n",
       " 'may',\n",
       " 'copy',\n",
       " 'it',\n",
       " ',',\n",
       " 'give',\n",
       " 'it',\n",
       " 'away',\n",
       " 'or\\\\r',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'re-use\",\n",
       " 'it',\n",
       " 'under',\n",
       " 'the',\n",
       " 'term',\n",
       " 'of',\n",
       " 'the',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'license',\n",
       " 'included\\\\r',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'with\",\n",
       " 'this',\n",
       " 'ebook',\n",
       " 'or',\n",
       " 'online',\n",
       " 'at',\n",
       " 'www.gutenberg.org\\\\r',\n",
       " \"'\",\n",
       " ']']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag   \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()  #lemmatization function\n",
    "\n",
    "def penn2morphy(penntag):\n",
    "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
    "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
    "                  'VB':'v', 'RB':'r'}\n",
    "    try:\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return 'n' \n",
    "    \n",
    "def lemmatize_sent(text): \n",
    "    # Text input is string, returns lowercased strings.\n",
    "    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) \n",
    "            for word, tag in pos_tag(word_tokenize(text))]\n",
    "\n",
    "lemmatize_sent(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f37262e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent_introduction:\n",
      "\n",
      "You may copy it, give it away or\\r', 're-use it under the terms of the Project Gutenberg License included\\r', 'with this eBook or online at www.gutenberg.org\\r'] \n",
      "\n",
      "Lemmatized and removed stopwords:\n",
      "\n",
      "['copy', 'give', 'or\\\\r', \"'re-use\", 'term', 'project', 'gutenberg', 'license', 'included\\\\r', \"'with\", 'ebook', 'online', 'www.gutenberg.org\\\\r']\n"
     ]
    }
   ],
   "source": [
    "print('sent_introduction:\\n')\n",
    "print(sent, '\\n')\n",
    "print('Lemmatized and removed stopwords:\\n')\n",
    "print([word for word in lemmatize_sent(sent) \n",
    "       if word not in stoplist_combined\n",
    "       and not word.isdigit() ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "008d0e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Input: str, i.e. document/sentence\n",
    "    # Output: list(str) , i.e. list of lemmas\n",
    "    return [word for word in lemmatize_sent(text) \n",
    "            if word not in stoplist_combined\n",
    "            and not word.isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b995e6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['introduces',\n",
       " 'student',\n",
       " 'fundamental',\n",
       " 'concept',\n",
       " 'state',\n",
       " 'challenge',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'overview',\n",
       " 'history',\n",
       " 'context',\n",
       " 'ai',\n",
       " 'develop',\n",
       " 'describes',\n",
       " 'concept',\n",
       " 'intelligence',\n",
       " 'agent',\n",
       " 'relate',\n",
       " 'environment',\n",
       " 'learning',\n",
       " 'knowledge',\n",
       " 'key',\n",
       " 'component',\n",
       " 'develop',\n",
       " 'intelligent',\n",
       " 'agent',\n",
       " 'ground',\n",
       " 'student',\n",
       " 'understanding',\n",
       " 'probabilistic',\n",
       " 'symbolic',\n",
       " 'reasoning',\n",
       " 'relate',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'relate',\n",
       " 'language',\n",
       " 'computer',\n",
       " 'vision',\n",
       " 'cognitive',\n",
       " 'architecture',\n",
       " 'representation',\n",
       " 'agent',\n",
       " '’',\n",
       " 'environment',\n",
       " 'cover',\n",
       " 'groundwork',\n",
       " 'continue',\n",
       " 'applied',\n",
       " 'machine',\n",
       " 'intelligence',\n",
       " 'program']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EAI6000_course_Description='Introduces students to the fundamental concepts, state and challenges of Artificial Intelligence. Brief overview of the history and context in which AI has developed. Describes how the concept of an intelligence agent relates to its environment and how learning and knowledge are key components in developing intelligent agents. Grounds students in an understanding of probabilistic and symbolic reasoning and how they relate to machine learning. Relates language and computer vision to cognitive architecture and representation of an agent’s environment. Covers the groundwork necessary for continuing to further courses in the Applied Machine Intelligence program.'\n",
    "preprocess_text(EAI6000_course_Description) #make an example "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bc12c6",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "75d799a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ANACONDA\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", '``', 'ai', 'ca', \"n't\", 'sha', 'wo'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'copy': 2,\n",
       " 'give': 4,\n",
       " 'or\\\\r': 9,\n",
       " \"'re-use\": 0,\n",
       " 'terms': 11,\n",
       " 'project': 10,\n",
       " 'gutenberg': 5,\n",
       " 'license': 7,\n",
       " 'included\\\\r': 6,\n",
       " \"'with\": 1,\n",
       " 'ebook': 3,\n",
       " 'online': 8,\n",
       " 'www.gutenberg.org\\\\r': 12}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from io import StringIO\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "with StringIO('\\n'.join([sent])) as fin:\n",
    "    # Create the vectorizer\n",
    "    count_vect = CountVectorizer(stop_words=stoplist_combined,\n",
    "                                 tokenizer=word_tokenize)\n",
    "    count_vect.fit_transform(fin)\n",
    "    \n",
    "count_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c6555788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x13 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 13 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.transform([sent])#To vectorize any new sentences, we use CountVectorizer.transform()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "009969ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['copy', 'give', 'or\\\\r', \"'re-use\", 'term', 'project', 'gutenberg', 'license', 'included\\\\r', \"'with\", 'ebook', 'online', 'www.gutenberg.org\\\\r']\n",
      "\n",
      "Vocab: (\"'re-use\", \"'with\", 'copy', 'ebook', 'give', 'gutenberg', 'included\\\\r', 'license', 'online', 'or\\\\r', 'project', 'terms', 'www.gutenberg.org\\\\r')\n",
      "\n",
      "Matrix/Vectors:\n",
      " [[1 1 1 1 1 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter      #view the matrix\n",
    "\n",
    "# Print the words sorted by their index\n",
    "words_sorted_by_index, _ = zip(*sorted(count_vect.vocabulary_.items(), key=itemgetter(1)))\n",
    "\n",
    "print(preprocess_text(sent))\n",
    "\n",
    "print()\n",
    "print('Vocab:', words_sorted_by_index)\n",
    "print()\n",
    "print('Matrix/Vectors:\\n', count_vect.transform([sent]).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e929e645",
   "metadata": {},
   "source": [
    "## task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fc1bed4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "Encode = preprocessing.LabelEncoder()\n",
    "vect = CountVectorizer()\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "NB = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7eb10ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response_id</th>\n",
       "      <th>class</th>\n",
       "      <th>response_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>response_1</td>\n",
       "      <td>not_flagged</td>\n",
       "      <td>I try and avoid this sort of conflict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>response_2</td>\n",
       "      <td>flagged</td>\n",
       "      <td>Had a friend open up to me about his mental ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>response_3</td>\n",
       "      <td>flagged</td>\n",
       "      <td>I saved a girl from suicide once. She was goin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>response_4</td>\n",
       "      <td>not_flagged</td>\n",
       "      <td>i cant think of one really...i think i may hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>response_5</td>\n",
       "      <td>not_flagged</td>\n",
       "      <td>Only really one friend who doesn't fit into th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  response_id        class                                      response_text\n",
       "0  response_1  not_flagged              I try and avoid this sort of conflict\n",
       "1  response_2      flagged  Had a friend open up to me about his mental ad...\n",
       "2  response_3      flagged  I saved a girl from suicide once. She was goin...\n",
       "3  response_4  not_flagged  i cant think of one really...i think i may hav...\n",
       "4  response_5  not_flagged  Only really one friend who doesn't fit into th..."
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot = pd.read_csv(r\"D:\\1\\EAI6000\\5\\chat-bot.csv\",usecols=['response_id','class','response_text'])\n",
    "chatbot.head(5) #This dataset flags some 'dangerous words'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c655322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot['Label'] = Encode.fit_transform(chatbot['class']) \n",
    "x = chatbot.response_text\n",
    "y = chatbot.Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "762f2077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x,y,random_state=1)#split data and build model\n",
    "x_train_dtm = vect.fit_transform(x_train)\n",
    "x_test_dtm = vect.transform(x_test)\n",
    "NB.fit(x_train_dtm,y_train)\n",
    "y_predict = NB.predict(x_test_dtm)\n",
    "metrics.accuracy_score(y_test,y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9017df6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82950e67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11abf114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddde3ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee0c022",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
